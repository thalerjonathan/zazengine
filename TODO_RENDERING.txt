Refactor remaining shaders
	-> get rid of todos
	-> clear in-out interface-blocks
	-> use layout(binding=0) on uniform samplers
		-> is always constant anyway => can set it compile-time constan in shaders!
	-> naming conventions
		-> read OpenGL Insights Chapter 2

For Lighting-Stage
	-> get rid of immediate-depth buffer usage until post-processing stage!

	-> write blog
		-> https://www.opengl.org/discussion_boards/showthread.php/177724-Reconstructing-depth-from-depth-buffer-texture

	-> understand math behind
		-> normal mapping
		-> reconstructing eye coord
		-> encoding/decoding normals

	-> directional-light shadow-artifacts?
	-> shadow map sampling: cube-map artifacts?

For Transparency-Stage
	-> need to be able to render from an aribtrary view point into aribtrary 2d target with different resolution: warning the projection matrix & viewport must match
		-> need 2 render-targets: planar & environmental (cube): renderFrameInternal( Viewer* viewer, RenderTarget* gatherTarget );
			-> will attach the gatherTarget to color-attachment 4
			-> when return color-attachment 4 will hold the rendered scene from this point of view
			-> take care not to render the viewer itself too (artifacts)
		-> then implementing planar-reflections & refractions like water or dynamic environmental mapping with reflection & refraction is no problem anymore
			-> http://stackoverflow.com/questions/9841863/reflection-refraction-with-chromatic-aberration-eye-correction
				-> simulate chromatic abberration (see Orange Book)
				-> need to simulate different wavelengths for red, green and blue channels
			-> for planar reflections&refractions:
				1. blit rendered scene to intermediate fb and system fb
				2. call renderFrameInternal with the correct viewing set-up
				3. then bind the gather-target
				4. run shader to do reflection & refraction using the original scene for refraction and the 2nd view-rendered scene for reflections and render to system framebuffer
			-> for cube environmental  reflections&refractions:
				1. blit rendered scene to system FB
				2. for each cube-face call renderFrameInternal with the correct viewing and the cube-map face
				3. bind the cube-map
				4. run shader to do reflection & refraction using the cube-map and render to system framebuffer

	-> OLD: environment dynamic cube-mapping
		-> need very generic renderFrame-method which allows to be called recursively
			-> need for every recursion level one CubeMap-FBO
			-> need to generify much of the code of the renderer-loop
			-> pass in some render-Context struct 
				-> holds recursion-level
				-> holds render-target where to render or null when to default framebuffer
				-> holds flag if shadow-rendering in environment-rendering
				-> holds flag if transparency-rendering in environment-rendering
				-> holds flag if interreflection-rendering in environment-rendering
				-> holds max recursion depth
		-> new material-type:
			<material name="Glass">
				<type id="ENVIRONMENTAL">
				  <reflection>
					... params
				  </reflection>

				  <refraction>
					... params
				  </refraction>
      
				  <diffuseColor file="cross_diffuse.jpg"/>
				  <color r="0.54" g="0.89" b="0.63"/>
				</type>
			  </material>
		-> material subclasses
			-> environment holds cube-map and params for shaders. it knows how often update cube-map per second

For Software-Architecture in Renderer
	-> draw-calls are very expensive in my current architecture
	-> split into frontend & backend
		-> issue commands: static objects, no new -> stateless
		-> issuing commands is api independent but api must have a corresponding functionality

Msaa
	-> do sampling in geometry fragment-shader (centroid)
	-> dual source blending?
	
Water-Rendering
	-> waves
		-> tesselation
		-> geometry-shader
	-> fresnel
	-> refraction
	-> reflections

Post-Processing in Screen-Space
	-> HDR
	-> Bloom
	-> Tone-Mapping
	-> Depth Of Field

Particle System
	-> techniques
		-> transform-feedback rendering
		-> compute-shaders ?
		-> tesselation-shaders ?
		-> geometry-shaders ?
	-> gravity
	-> global physically simulated
		-> bounce off other geometry
	-> local physically simulated
		-> no bouncing off other geometry
	-> poing-lights
		-> glowing huge number of point-light particles illuminate dark scene
			-> no shadow-casting
	-> no poing-lights
		-> don't illuminate scene
	
Text-rendering
	-> use OGLFT

Console
	-> implement console for input of text and commands

Menu-selection
	-> implement menu

Explore Bindless G-Buffer also called layered rendering
	-> more cleaner solution
	-> can be extended more easily
	-> faster
	-> order indipendent transparency (OIT) in deferred rendering!!!
	-> http://openglinsights.com/bendingthepipeline.html#EfficientLayeredFragmentBufferTechniques

Lighting
	-> make z-bias configurable/light - directional lights need a higher bias
	-> make filter-width of PCF configurable and filtering on/off possible

Need generic effect/material functionality
	-> no need to set input or output indices in shaders, can be done with layout ( location = ... )
		-> remove glGetFragDataLocation, glBindFragDataLocation, glGetAttribLocation, glBindAttribLocation
		-> remove glUniform1i and glUniformBlockBinding
	-> on top of program- & shader-management
		-> implement #include in shaders: parse loaded string and replace
			-> GL_ARB_shading_language_include is an extension that provides an #include directive to GLSL
		-> problem: compiler error generate other line numbers!
	-> use shaders-subroutines
		-> compile all possible material stuff in the shader and select by setting propriate subroutine
			-> needs includes
	-> based upon effect-files like directx/cg shader effects
	-> need some semantics on uniforms
	-> custom shader program for material programming
	-> problem: shaders-effect architecture from 2003 is based upon forward-rendering, we need now to plug it into deferred rendering
		-> 2 stages: geometry & ligting => potential up to 4 shaders/material: vertex&fragment in geometry & lighting
			-> BUT: vertex in geometry- & lighting-stage is very hard wired to deferred rendering
	-> important FIRST finish all special algorithms: shadow mapping, normal mapping, transparency, lighting, reflections, MSAA, ... (?)
	-> can be seen as a generic framework to implement BRDFs (?)
		-> sub-surface-scattering
		-> cook torrance sparrow
		-> subsurface scattering
			-> use depth-map to calculate distance the light traveled: http://http.developer.nvidia.com/GPUGems/gpugems_ch16.html
				-> 1. lookup depth of light
				-> 2. transform position into light-space
				-> 3. calculate distance from position to light
				-> 4. transform depth of light to distanceo to light
			-> or use fake hack: http://machinesdontcare.wordpress.com/2008/10/29/subsurface-scatter-shader/ 

Transparency-Rendering (see GPU Gems 2 Page 295)
	-> Differentiate between normal transparency and refractive transparency
		-> normal transparency can be rendered at the very last using normal opengl blending
			-> no overhead of copying depth, reading background and combining results into new background
		-> split into two types: TRANSPARENT_BLENDING & TRANSPARENT_REFRACTION
	
	-> Do Colored Shadow-Rendering for Transparent objects
		-> render transparent object in shadow-map generation AND write color of transparent object to additional color-texture in shadow-fbo
			-> optimization: when material is transparent write to 1.0 to alpha-channel, else 0.0
			=> need additional render-target in shadow-fbo: shadow-color target
		-> during light-rendering: when fragment in shadow do an additional lookup into the shadow-color texture to get the shadow-color for this fragment
			-> optimization: only do additional lookup when alpha-channel is 1.0
		-> for better quality: do multisampling texture lookups

GLFW Context
	-> Allow Toggle fullscreen
		-> reinit window & render-context!
			=> need to reinig glew, programs, textures,... (?)
			=> lot of work to do, because everything needs to be re-init-able
		-> for this to work we need to split creation of objects which interact with opengl and the initialization of those object with opengl-state
			-> initialization must happen in a specific order. faster buffers need to be created first,... 

Optimization WHEN RENDERER HAS FINISHED, NOT BEFORE!!!
	-> instanced rendering
	-> texture-compression
	-> use copies of uniform buffers e.g. for each light a buffer
		-> this reduces the update just to binding the buffer ot the shader and not uploading values to fields every frame
		-> No more bindbufferbase of uniformblocks in renderer - do it after programs are linked in uniformmanagement
	-> Culling
		-> View-space culling using camera
		-> implement CHC++
	-> create most important render-targets (g-buffer) & depth-buffers (G-buffer, shadow-maps,...) first
	-> DO IN-DEPTH PERFORMANCE-MEASUREMENTS (premature optimiziation is the root of all evil!)
		-> use nvidia nsight
	-> use EXT_direct_state_access
		-> no more need to bind textures, ...
		-> http://www.g-truc.net/post-0363.html#menu
		-> http://www.opengl.org/registry/specs/EXT/direct_state_access.txt
	-> EXT_swap_control_tear ?
	-> ARB_sampler_objects
	-> Do Culling
		-> View-Frustum
		-> Occlusion Culling using queries (like CHC++) 
		-> Culling library (Umbra 3) ?
	-> Experiment with different render-target formats for performance
	-> really all calls necessary e.g. always bindBuffer, ... ? are some states stored in buffers?
	-> opengl 4.3 allows for READ & WRITE of textures (images) at the same time
		-> use for rendering transparent-glass
	-> do depth-sorting for ALL objects in scene
		-> calculate model-view once and then reuse in geometry-rendering
