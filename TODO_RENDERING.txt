Read
	-> opengl insights
		Best Practices, Current Knowledge & Optimization
		-> chapter 5 shader interfaces
		-> chapter 25 performance reducing driver-calls

		Future Tech
		-> chapter 14 layered textures rendering pipeline
		-> chapter 19 massive shadow-casting ... layered rendering
		-> chapter 20 efficient layered fragment buffer techniques
		-> chapter 21 programmable vertex pulling
		-> chapter 23 performance tuning tile-based architectures

Todo
	-> shadows of environment transparents ??
		-> make flag: for all objects if they cast a shadow or not
			-> then a shadow-map could have no affecting objects => no need for shadowing => don't render => leave for further implementations 
	-> there is no need to render the shadow-maps multiple times because it won't change => keep all shadow-maps in memory but then no sharing can be used
		-> make a scene-config in xml: share shadow-maps on/off.
			-> if on => shadow-maps need to be rerendered every time
			-> if off => shadow-maps can be reused
			=> user can decide tradeoff
	-> scaled objects seem to be fucked up
	-> pull configuration-parameters in the material-class and make it configurable through xml-config

Transparency-Stage
	-> implement planar reflection/refraction
		-> arbitary reflection around a plane, not only origin with +y
		-> waves
			-> tesselation
			-> geometry-shader
			-> fresnel
			-> refraction
			-> reflections
	-> Do Colored Shadow-Rendering for Transparent objects
		-> render transparent object in shadow-map generation AND write color of transparent object to additional color-texture in shadow-fbo
			-> optimization: when material is transparent write to 1.0 to alpha-channel, else 0.0
			=> need additional render-target in shadow-fbo: shadow-color target
		-> during light-rendering: when fragment in shadow do an additional lookup into the shadow-color texture to get the shadow-color for this fragment
			-> optimization: only do additional lookup when alpha-channel is 1.0
		-> for better quality: do multisampling texture lookups

Blog
	-> write lighting-stage draft https://www.opengl.org/discussion_boards/showthread.php/177724-Reconstructing-depth-from-depth-buffer-texture
	-> get rid of immediate-depth buffer usage until post-processing stage!
		-> it seems we cannot get rid of it without attaching/detaching targets all the time
		-> FrameBuffer-Class is a mess... refactor it when environmental cube & planar is finished
	-> understand math behind
		-> normal mapping
			1: why do we need to perform lighting in tangent-space? Could we not transform the local normal-vector in the geometry-stage to view-space?
			2. what are we actually doing here with those dot-products?
			3. what is the math behind normal-mapping?
		-> reconstructing eye coord
	-> directional-light shadow-artifacts?
	-> point-light shadow-artifacts?	
	-> info about shadow-texture lookup
		// IMPORTANT: we are only in clip-space, need to divide by w to reach projected NDC.
		// normally the forward-rendering shadow-mapping calculates the shadow-coord
		// in the vertex-shader, which is not possible in the deferred renderer without using 
		// an additional render-target. in forward-rendering between the vertex-shader
		// and the fragment-shader where the shadow-map lookup happens
		// interpolation is carried out by the fixed-function but no perspective division
		// the shadowcoord is in our case already multiplied with the projection-transform
		// but we still need to do the projective-division to reach the according space 0-1
		// because this is not done when applying the projection transform
		// ADDITION: either use 	
		//		vec3 shadowCoordPersp = shadowCoord.xyz / shadowCoord.w; with texture( ShadowPlanarMap, shadowCoordPersp ) lookup 
		//		OR use textureProj( ShadowPlanarMap, shadowCoord ); directly
		
		// IMPORTANT: because we installed a compare-function on this shadow-sampler
		// we don't need to compare it anymore to the z-value of the shadow-coord

		// IMPORTANT: directionaly light uses orthogonal shadow-map so 
		// transformation of position to shadow-coord is orthgonal projection too
		// so no need for perspective lookup (or divide) because orthogonal 
		// projection has 1 at w so there won't be a foreshortening of values
		// IMPORTANT: because we installed a compare-function on this shadow-sampler
		// we don't need to compare it anymore to the z-value of the shadow-coord

		// for shadow-mapping we need to transform the position of the fragment to light-space
		// before we can apply the light-space transformation we first need to apply
		// the inverse view-matrix of the camera to transform the position back to world-coordinates (WC)
		// note that world-coordinates is the position after the modeling-matrix was applied to the vertex



Msaa
	-> do sampling in geometry fragment-shader (centroid)
	-> dual source blending?

Post-Processing in Screen-Space
	-> HDR
	-> Bloom
	-> Tone-Mapping
	-> Depth Of Field

Particle System
	-> techniques
		-> transform-feedback rendering
		-> compute-shaders ?
		-> tesselation-shaders ?
		-> geometry-shaders ?
	-> gravity
	-> global physically simulated
		-> bounce off other geometry
	-> local physically simulated
		-> no bouncing off other geometry
	-> poing-lights
		-> glowing huge number of point-light particles illuminate dark scene
			-> no shadow-casting
	-> no poing-lights
		-> don't illuminate scene
	
Material
	-> Depth-Map SSS from OglPlus example 28
		-> problem: DepthLightMap must store distance-from light values in NDC => no more depth-comparison function possible, switch all shadow-maps
		-> cube-map is handled different than planar
		-> directional-light is another special case too because parallel-projection
		vec3 calculateDepthSSSMaterial( vec3 baseColor, vec3 fragPosEC, vec3 normalEC, vec3 lightDirEC )
		{
			vec3 eyeDirToFragEC = normalize( -fragPosEC );

			vec4 fragPosWS = Camera.modelMatrix * vec4( fragPosEC, 1.0 );
			vec4 fragPosLightClipSpace = Light.viewProj * fragPosWS;

			float LightDist = fragPosLightClipSpace.z / fragPosLightClipSpace.w;
			vec3 Normal = normalize(normalEC);
			vec3 LightDir = normalize(lightDirEC);
			vec3 LightRefl = reflect(-LightDir, Normal);
			vec3 ViewDir = normalize(eyeDirToFragEC);

			float inv_w = 1.0/fragPosLightClipSpace.w;

			float Depth = 0.0;
			for(int s=0; s!=DepthSamples; ++s)
			{
				vec2 SampleCoord = DepthOffs[s]+fragPosLightClipSpace.xy;
				SampleCoord *= inv_w;
				SampleCoord *= 0.5;
				SampleCoord += 0.5;
				float Sample = texture(DepthLightMap, SampleCoord).r;
				if(Sample < 0.95)
					Depth += Sample;
				else Depth += 0.5;
			}
			Depth *= InvDepthSamples;

			float Ambi = 0.15;
			float BkLt = (dot(-LightDir, ViewDir)+3.0)*0.25;
			float SuSS = pow(abs(Depth-LightDist), 2.0)*BkLt*1.2;
			float Shdw = min(pow(abs(Depth-LightDist)*2.0, 8.0), 1.0);
			float Diff  = sqrt(max(dot(LightDir, Normal)+0.1, 0.0))*0.4;
			float Spec  = pow(max(dot(LightRefl, ViewDir), 0.0), 64.0);

			vec3 fragColor = (Ambi + Shdw*Diff + SuSS) * baseColor;
			fragColor += Shdw*Spec * vec3(1.0, 1.0, 1.0);
	
			return fragColor;
		}

Text-rendering
	-> use OGLFT

Console
	-> implement console for input of text and commands

Menu-selection
	-> implement menu

For Software-Architecture in Renderer
	-> FrameBuffer-Class is a mess... refactor it
	-> check if this bool-for-every method makes really sense 
	-> draw-calls are very expensive in my current architecture
	-> split into frontend & backend
		-> issue commands: static objects, no new -> stateless
		-> issuing commands is api independent but api must have a corresponding functionality
	-> GLFW Context
		-> Allow Toggle fullscreen
			-> reinit window & render-context!
				=> need to reinig glew, programs, textures,... (?)
				=> lot of work to do, because everything needs to be re-init-able
			-> for this to work we need to split creation of objects which interact with opengl and the initialization of those object with opengl-state

				-> initialization must happen in a specific order. faster buffers need to be created first,... 
Need generic effect/material functionality
	-> no need to set input or output indices in shaders, can be done with layout ( location = ... )
		-> remove glGetFragDataLocation, glBindFragDataLocation, glGetAttribLocation, glBindAttribLocation
		-> remove glUniform1i and glUniformBlockBinding
	-> on top of program- & shader-management
		-> implement #include in shaders: parse loaded string and replace
			-> GL_ARB_shading_language_include is an extension that provides an #include directive to GLSL
		-> problem: compiler error generate other line numbers!
	-> use shaders-subroutines
		-> compile all possible material stuff in the shader and select by setting propriate subroutine
			-> needs includes
	-> based upon effect-files like directx/cg shader effects
	-> need some semantics on uniforms
	-> custom shader program for material programming
	-> problem: shaders-effect architecture from 2003 is based upon forward-rendering, we need now to plug it into deferred rendering
		-> 2 stages: geometry & ligting => potential up to 4 shaders/material: vertex&fragment in geometry & lighting
			-> BUT: vertex in geometry- & lighting-stage is very hard wired to deferred rendering
	-> important FIRST finish all special algorithms: shadow mapping, normal mapping, transparency, lighting, reflections, MSAA, ... (?)
	-> can be seen as a generic framework to implement BRDFs (?)
		-> sub-surface-scattering
		-> cook torrance sparrow
		-> subsurface scattering
			-> use depth-map to calculate distance the light traveled: http://http.developer.nvidia.com/GPUGems/gpugems_ch16.html
				-> 1. lookup depth of light
				-> 2. transform position into light-space
				-> 3. calculate distance from position to light
				-> 4. transform depth of light to distanceo to light
			-> or use fake hack: http://machinesdontcare.wordpress.com/2008/10/29/subsurface-scatter-shader/ 

Optimization WHEN RENDERER HAS FINISHED, NOT BEFORE!!!
	-> instanced rendering
	-> texture-compression
	-> use copies of uniform buffers e.g. for each light a buffer
		-> this reduces the update just to binding the buffer ot the shader and not uploading values to fields every frame
		-> No more bindbufferbase of uniformblocks in renderer - do it after programs are linked in uniformmanagement
	-> Culling
		-> View-space culling using camera
		-> implement CHC++
	-> create most important render-targets (g-buffer) & depth-buffers (G-buffer, shadow-maps,...) first
	-> DO IN-DEPTH PERFORMANCE-MEASUREMENTS (premature optimiziation is the root of all evil!)
		-> use nvidia nsight
	-> use EXT_direct_state_access
		-> no more need to bind textures, ...
		-> http://www.g-truc.net/post-0363.html#menu
		-> http://www.opengl.org/registry/specs/EXT/direct_state_access.txt
	-> EXT_swap_control_tear ?
	-> ARB_sampler_objects
	-> Do Culling
		-> View-Frustum
		-> Occlusion Culling using queries (like CHC++) 
		-> Culling library (Umbra 3) ?
	-> really all calls necessary e.g. always bindBuffer, ... ? are some states stored in buffers?
	-> opengl 4.3 allows for READ & WRITE of textures (images) at the same time
		-> use for rendering transparent-glass
	-> do depth-sorting for ALL objects in scene
		-> calculate model-view once and then reuse in geometry-rendering
